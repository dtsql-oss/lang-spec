{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DTSQL Documentation # DTSQL ( \u201cDeclarative Time Series Query Language\u201d ) is a high-level domain-specific language geared towards querying time series data (sensor data) declaratively. This documentation provides a specification of DTSQL, i.e., its language features, their syntax and semantics as well as illustrating example queries. Fundamental Language Concepts # In general, a DTSQL query consists of five components: A set of samples . They represent scalar values expressed by aggregation functions (e.g., avg , sum ). These values may be referenced/reused in other query components (e.g., filters or events) or also serve as overall query result. A filter specification that excludes irrelevant data points of the input time series from the query evaluation. A set of events . They are a declarative specification of periods (intervals) to be captured during query evaluation. A selection component which defines temporal relations between detected event periods (e.g., precedes , follows ). This allows one to express that events ought to occur in a certain sequence in order to be part of the query result. A yield statement which ultimately determines the query result. The actual value of this statement decides whether the result is made up of data points, intervals or sample values. All components, except for yield , are optional. More specifically, this means: Note: Required/Optional DTSQL Components the samples component may be omitted the filter component may be omitted the events component may be omitted the selection component may be omitted the yield component is mandatory Temporal Concepts and Result Objects # DTSQL utilizes notions such as time series, data points, time intervals (periods) and points in time in general. They are mostly encapsulated in their own respective data types. All of them are explained in detail in the section dedicated to data structures . Providing Input Data # A main goal of DTSQL is keeping it generic and independent of a concrete underlying storage mechanism. Therefore, queries expressed in this language operate on a canonical representation of time series. This representation is explained in the section about the data structures . In addition to that, the section about query API explains how arbitrary data sources may be connected to and queried using DTSQL. Example Query # Example: Query Utilizing Core Language Features WITH SAMPLES: avg() AS globalArithmeticMean, min(\"2022-05-28T14:15:00Z\", \"\") AS localMinimum APPLY FILTER: AND(NOT(before(\"2022-05-28T14:15:00Z\"))) USING EVENTS: AND(gt(globalArithmeticMean)) FOR (30,] seconds AS aboveAvg, AND(around(rel, localMinimum, 25)) FOR [2,] minutes AS aroundMin SELECT PERIODS: (aboveAvg follows aroundMin WITHIN [,4) seconds) YIELD: all periods The \u201cteaser\u201d query above has the following meaning: Info: Explanation of Example Query declare two samples a global average (considering the entire input time series) a local minimum value (from 2022-05-28T14:15:00Z until the end of the input time series) filter out data points which were measured before the very same point in time 2022-05-28T14:15:00Z characterize two kinds of periods (events) those with values consistently above the global average for more than 30 seconds those with ony values within a \u00b125 % range of the previously declared local minimum for at least two minutes select composite (merged) periods capturing binary event sequences where, within less than four seconds, a periods of the first event follows (occurs after) a period of the second event yield these captured composite periods as final query result","title":"Getting Started"},{"location":"#dtsql-documentation","text":"DTSQL ( \u201cDeclarative Time Series Query Language\u201d ) is a high-level domain-specific language geared towards querying time series data (sensor data) declaratively. This documentation provides a specification of DTSQL, i.e., its language features, their syntax and semantics as well as illustrating example queries.","title":"DTSQL Documentation"},{"location":"#fundamental-language-concepts","text":"In general, a DTSQL query consists of five components: A set of samples . They represent scalar values expressed by aggregation functions (e.g., avg , sum ). These values may be referenced/reused in other query components (e.g., filters or events) or also serve as overall query result. A filter specification that excludes irrelevant data points of the input time series from the query evaluation. A set of events . They are a declarative specification of periods (intervals) to be captured during query evaluation. A selection component which defines temporal relations between detected event periods (e.g., precedes , follows ). This allows one to express that events ought to occur in a certain sequence in order to be part of the query result. A yield statement which ultimately determines the query result. The actual value of this statement decides whether the result is made up of data points, intervals or sample values. All components, except for yield , are optional. More specifically, this means: Note: Required/Optional DTSQL Components the samples component may be omitted the filter component may be omitted the events component may be omitted the selection component may be omitted the yield component is mandatory","title":"Fundamental Language Concepts"},{"location":"#temporal-concepts-and-result-objects","text":"DTSQL utilizes notions such as time series, data points, time intervals (periods) and points in time in general. They are mostly encapsulated in their own respective data types. All of them are explained in detail in the section dedicated to data structures .","title":"Temporal Concepts and Result Objects"},{"location":"#providing-input-data","text":"A main goal of DTSQL is keeping it generic and independent of a concrete underlying storage mechanism. Therefore, queries expressed in this language operate on a canonical representation of time series. This representation is explained in the section about the data structures . In addition to that, the section about query API explains how arbitrary data sources may be connected to and queried using DTSQL.","title":"Providing Input Data"},{"location":"#example-query","text":"Example: Query Utilizing Core Language Features WITH SAMPLES: avg() AS globalArithmeticMean, min(\"2022-05-28T14:15:00Z\", \"\") AS localMinimum APPLY FILTER: AND(NOT(before(\"2022-05-28T14:15:00Z\"))) USING EVENTS: AND(gt(globalArithmeticMean)) FOR (30,] seconds AS aboveAvg, AND(around(rel, localMinimum, 25)) FOR [2,] minutes AS aroundMin SELECT PERIODS: (aboveAvg follows aroundMin WITHIN [,4) seconds) YIELD: all periods The \u201cteaser\u201d query above has the following meaning: Info: Explanation of Example Query declare two samples a global average (considering the entire input time series) a local minimum value (from 2022-05-28T14:15:00Z until the end of the input time series) filter out data points which were measured before the very same point in time 2022-05-28T14:15:00Z characterize two kinds of periods (events) those with values consistently above the global average for more than 30 seconds those with ony values within a \u00b125 % range of the previously declared local minimum for at least two minutes select composite (merged) periods capturing binary event sequences where, within less than four seconds, a periods of the first event follows (occurs after) a period of the second event yield these captured composite periods as final query result","title":"Example Query"},{"location":"data-structures/data-structures/","text":"Data Structures # This page explains how DTSQL represents different kinds of data it handles. Time # Some features provided by DTSQL reference specific points in time (e.g., for local aggregates) or intervals (e.g., for temporal aggregates). The representation of time is not dictated by DTSQL\u2019s grammar and thus, generally, implementation-specific. However, for the sake of reusing existing standards, it is recommended to utilize the ISO 8601 standard 1 . Furthermore, the implementation must be capable of the following kinds of operations on date/time variables: Extracting components from a date/time variable, i.e., year, month, day, hour, minute, second, millisecond. Example Given the ISO 8601 timestamp 2022-09-18T16:42:23.254+01:00 , the implementation should be able to extract the following components: year: 2022 month: 9 day: 18 hour: 16 minute: 42 second: 23 millisecond: 254 In words, this timestamp describes September 18th 2022 at 16:42:23 and 254 milliseconds in a timezone which is one hour ahead of UTC. Calculating the difference between two date/time variables in (fractional) weeks, days, hours, minutes, seconds or milliseconds. Example Given the ISO 8601 timestamps 2022-09-18T08:42:23.254Z and 2022-09-18T22:42:23.254Z , the unt-dependent time differences are as follows: week: 0,0833 day: 0.5833 hour: 14 minute: 840 second: 50 400 millisecond: 50 400 000 Again, the ability to conduct this calculation in a timezone-aware manner (e.g., producing different results for timestamps with non-equal timezone offsets) is not dictated by the DTSQL specification and depends on the implementation. The diagrams below visualize the features strictly required of a date/time representation in a DTSQL implementation. classDiagram class Date { +extract(u: TimeUnit) Integer +getDuration(d1: Date, d2: Date, u: TimeUnit)$ Real } class TimeUnit{ <<enumeration>> YEAR MONTH DAY HOUR MINUTE SECOND MILLISECOND } Data Points # A data point represents a measurement (\u201csensor log\u201d). It consists of both the time at which it was recorded and the value that was observed. Additional operations (e.g., formatting the value up to a certain amount of precision or number of significant digits) are valid, but not required. This is visualized below. classDiagram class DataPoint { +timestamp : Date +value : Real } Time Series # Time series are made up of several measurements, i.e., data points. Their size or length is equal to the number of data points they comprise. It is also possible to retrieve a subset of a time series from an interval . The diagram below illustrates the structure of time series. classDiagram class TimeSeries { +dataPoints: DataPoint[] +getSize() Integer +get(idx: Integer) DataPoint +getSubSeries(pd: Period) TimeSeries } DTSQL stipulates two important assumptions about time series and their data points: Warning: Assumptions About Time Series The two assumptions below greatly facilitate designing algorithms for query evaluation modules. DTSQL implementations are not expected to ensure or verify them, they rather serve as preconditions of the input data. Time series are in ascending order with respect to the time component of their data points. Data points in a time series are unique with respect to their time component (i.e., there are no two data points with the same time component). This implies that the data points of a time series are strictly monotonically increasing with respect to their time components. Time Intervals # A time interval, interval, or period is a segment of time that is defined by a lower bound and upper bound. It may be used to capture a subset of a time series. Since one of DTSQL\u2019s core features is the detection of periods of interest, they may be equipped with an index that denotes their position in the sequence of detected periods. This is summarized in the image below. classDiagram class Period { +start : Date +end : Date +index : Integer +getLength(u: TimeUnit) TimeUnit +contains(dp: DataPoint) Boolean } Query Results # classDiagram class QueryResult <<interface>> QueryResult QueryResult <|-- Period QueryResult <|-- PeriodSet QueryResult <|-- DataPointSet QueryResult <|-- Scalar QueryResult <|-- ScalarSet Depending on the Yield Component of a DTSQL query, the result may take different shapes. The paragraphs below explain the composition of the result formats supported by DTSQL. Period # In case a single period is returned (e.g., the longest of all detected periods), the query result is an instance of the Period type introduced in Time Intervals . PeriodSet # If the result (potentially) comprises more than one period, then an instance of PeriodSet , as illustrated below, is returned. classDiagram class PeriodSet { +periods : Period[] +getLength() Integer +isEmpty() Boolean +get(idx: Integer) Period } DataPointSet # For instances when all data points are returned (either after filtering or contained by a detected period), then an object of DataPointSet is returned. This data structure is very similar to PeriodSet , as the diagram below shows. classDiagram class DataPointSet { +periods : DataPoint[] +getLength() Integer +isEmpty() Boolean +get(idx: Integer) DataPoint } Scalar # DTSQL may return the calculated values of samples. The Scalar wrapper type below encapsulates these concrete aggregate values. classDiagram class Scalar { +value : Real } ScalarSet # The ScalarSet structure displayed below is equivalent to Scalar for the case of results consisting of multiple scalar values. classDiagram class ScalarSet { +values : Real[] } ISO 8601 offers a wide array of possibilities to express date and time-related data. It is completely up to the specific DTSQL implementation to what extent these capabilities are supported (e.g., unix timestamps, week with weekday notation, time zones, ordinal dates, etc.). \u21a9","title":"Data Structures"},{"location":"data-structures/data-structures/#data-structures","text":"This page explains how DTSQL represents different kinds of data it handles.","title":"Data Structures"},{"location":"data-structures/data-structures/#time","text":"Some features provided by DTSQL reference specific points in time (e.g., for local aggregates) or intervals (e.g., for temporal aggregates). The representation of time is not dictated by DTSQL\u2019s grammar and thus, generally, implementation-specific. However, for the sake of reusing existing standards, it is recommended to utilize the ISO 8601 standard 1 . Furthermore, the implementation must be capable of the following kinds of operations on date/time variables: Extracting components from a date/time variable, i.e., year, month, day, hour, minute, second, millisecond. Example Given the ISO 8601 timestamp 2022-09-18T16:42:23.254+01:00 , the implementation should be able to extract the following components: year: 2022 month: 9 day: 18 hour: 16 minute: 42 second: 23 millisecond: 254 In words, this timestamp describes September 18th 2022 at 16:42:23 and 254 milliseconds in a timezone which is one hour ahead of UTC. Calculating the difference between two date/time variables in (fractional) weeks, days, hours, minutes, seconds or milliseconds. Example Given the ISO 8601 timestamps 2022-09-18T08:42:23.254Z and 2022-09-18T22:42:23.254Z , the unt-dependent time differences are as follows: week: 0,0833 day: 0.5833 hour: 14 minute: 840 second: 50 400 millisecond: 50 400 000 Again, the ability to conduct this calculation in a timezone-aware manner (e.g., producing different results for timestamps with non-equal timezone offsets) is not dictated by the DTSQL specification and depends on the implementation. The diagrams below visualize the features strictly required of a date/time representation in a DTSQL implementation. classDiagram class Date { +extract(u: TimeUnit) Integer +getDuration(d1: Date, d2: Date, u: TimeUnit)$ Real } class TimeUnit{ <<enumeration>> YEAR MONTH DAY HOUR MINUTE SECOND MILLISECOND }","title":"Time"},{"location":"data-structures/data-structures/#data-points","text":"A data point represents a measurement (\u201csensor log\u201d). It consists of both the time at which it was recorded and the value that was observed. Additional operations (e.g., formatting the value up to a certain amount of precision or number of significant digits) are valid, but not required. This is visualized below. classDiagram class DataPoint { +timestamp : Date +value : Real }","title":"Data Points"},{"location":"data-structures/data-structures/#time-series","text":"Time series are made up of several measurements, i.e., data points. Their size or length is equal to the number of data points they comprise. It is also possible to retrieve a subset of a time series from an interval . The diagram below illustrates the structure of time series. classDiagram class TimeSeries { +dataPoints: DataPoint[] +getSize() Integer +get(idx: Integer) DataPoint +getSubSeries(pd: Period) TimeSeries } DTSQL stipulates two important assumptions about time series and their data points: Warning: Assumptions About Time Series The two assumptions below greatly facilitate designing algorithms for query evaluation modules. DTSQL implementations are not expected to ensure or verify them, they rather serve as preconditions of the input data. Time series are in ascending order with respect to the time component of their data points. Data points in a time series are unique with respect to their time component (i.e., there are no two data points with the same time component). This implies that the data points of a time series are strictly monotonically increasing with respect to their time components.","title":"Time Series"},{"location":"data-structures/data-structures/#time-intervals","text":"A time interval, interval, or period is a segment of time that is defined by a lower bound and upper bound. It may be used to capture a subset of a time series. Since one of DTSQL\u2019s core features is the detection of periods of interest, they may be equipped with an index that denotes their position in the sequence of detected periods. This is summarized in the image below. classDiagram class Period { +start : Date +end : Date +index : Integer +getLength(u: TimeUnit) TimeUnit +contains(dp: DataPoint) Boolean }","title":"Time Intervals"},{"location":"data-structures/data-structures/#query-results","text":"classDiagram class QueryResult <<interface>> QueryResult QueryResult <|-- Period QueryResult <|-- PeriodSet QueryResult <|-- DataPointSet QueryResult <|-- Scalar QueryResult <|-- ScalarSet Depending on the Yield Component of a DTSQL query, the result may take different shapes. The paragraphs below explain the composition of the result formats supported by DTSQL.","title":"Query Results"},{"location":"data-structures/data-structures/#period","text":"In case a single period is returned (e.g., the longest of all detected periods), the query result is an instance of the Period type introduced in Time Intervals .","title":"Period"},{"location":"data-structures/data-structures/#periodset","text":"If the result (potentially) comprises more than one period, then an instance of PeriodSet , as illustrated below, is returned. classDiagram class PeriodSet { +periods : Period[] +getLength() Integer +isEmpty() Boolean +get(idx: Integer) Period }","title":"PeriodSet"},{"location":"data-structures/data-structures/#datapointset","text":"For instances when all data points are returned (either after filtering or contained by a detected period), then an object of DataPointSet is returned. This data structure is very similar to PeriodSet , as the diagram below shows. classDiagram class DataPointSet { +periods : DataPoint[] +getLength() Integer +isEmpty() Boolean +get(idx: Integer) DataPoint }","title":"DataPointSet"},{"location":"data-structures/data-structures/#scalar","text":"DTSQL may return the calculated values of samples. The Scalar wrapper type below encapsulates these concrete aggregate values. classDiagram class Scalar { +value : Real }","title":"Scalar"},{"location":"data-structures/data-structures/#scalarset","text":"The ScalarSet structure displayed below is equivalent to Scalar for the case of results consisting of multiple scalar values. classDiagram class ScalarSet { +values : Real[] } ISO 8601 offers a wide array of possibilities to express date and time-related data. It is completely up to the specific DTSQL implementation to what extent these capabilities are supported (e.g., unix timestamps, week with weekday notation, time zones, ordinal dates, etc.). \u21a9","title":"ScalarSet"},{"location":"query-api/query-api-overview/","text":"Query API # In order to ensure the generality of DTSQL, it operates on a canonical representation of time series (as already explained in the respective section about Time Series ). This allows developers to hook up any storage system containing time series data (CSV files, InfluxDB, SQL databases, web services, \u2026) to DTSQL. The paragraphs below explain both how canonical time series are queried and which interfaces are involved in obtaining such queryable time series. Query Service # The QueryService interface is very simple, providing only a single method that takes a time series and a DTSQL query and returns a query result (see also the section about Query Results ). Implementations should parse the query into an appropriate internal representation and then evaluate it based on the respective input time series. classDiagram class QueryService { <<interface>> +query(ts: TimeSeries, dtsqlQuery: String) QueryResult } Storage Services # Before feeding a canonical time series to a QueryService implementation, it has to be extracted from an arbitrary data source. This is possible via the StorageService , StorageConfig and StorageConfig interfaces. In general, StorageService provides data access, parameterized by a StorageConfig instance consisting of several StorageProperty values. The diagrams below visualize the members of each interface. Underneath them, dedicated content tabs provide a more in-depth view of these three interfaces, explaining their individual methods and depicting succinct examples of how they may be utilized in practice. classDiagram class StorageService~T, U extends StorageConfig~ { <<interface>> +initialize(svcCfg: U) +isInitialized() Boolean +load(lookupCfg: U) T[] +transform(loadedData: T[], transformCfg: U) TimeSeries +store(data: TimeSeries, persistCfg: U) } class StorageProperty { <<interface>> +getIdentifier() String +getType() Class~V~ } classDiagram class StorageConfig { <<interface>> +getSupportedProperties() StorageProperty[] +isPropertySet(prop: StorageProperty) Boolean +getProperty(prop: StorageProperty, [type: Class~V~]) V +setProperty(prop: StorageProperty, value: Object) Object +unsetProperty(prop: StorageProperty) Object } StorageService StorageConfig StorageProperty Represents an abstraction to be used to connect a custom storage mechanism to a Query Service instance. Info: Interface Method Descriptions T : The storage-specific (native) type of data retrieved by load . U : The configuration compatible with current StorageService implementation, must be a subtype of StorageConfig . initialize : Can be used for general setup specific to the storage (e.g., connect to database, check I/O availability, \u2026). isInitialized : Determines whether the service has been initialized; is a precondition for store , load and transform . load : Reads required data from the storage with parameters specified by lookupCfg into the native data type T . transform : Converts the data obtained from load into a TimeSeries instance according to the parameters represented by transformCfg . store : Is meant to store/persist a TimeSeries instance into the storage system. This operation basically conducts the inverse conversion of transform . This method is not needed in the majority of cases. Therefore, generally, it is valid not to implement this method. Example: CsvStorageService Implementation (Prose) For this example, we assume CSV file accesses are encapsulated by an third-party library. T : CsvRow , a data structure provided by the library that represents a line in a CSV file. U : CsvStorageConfig , a derived type of StorageConfig that is valid to feed to CsvStorageService (see second tab page for details). initialize : Since file streams are opened and closed at the moment of accessing CSV files, there is nothing like an underlying database connection or web session. Therefore, this method may be empty. isInitialized : For the reasons explained above, this method may always return true . load : The CsvStorageConfig instance lookupCfg contains parameters such as file path, field delimiter, and a number of lines to skip before parsing. Using those parameters, the CSV library can be instructed to load the desiredd data as CsvRow instance. transform : In order to convert a range of CsvRow instances to DataPoint objects (and subsequently a TimeSeries ), the transformCfg contains information such as index of date and value columns and date/time format to correctly parse time components of data points. store : If one were to implement this method, persistCfg should contain information such as destination path, field separators to use, format to serialize time components into and possibly an option to toggle create/append file modes. Represents a container for the configuration (properties) of a concrete StorageService . Info: Interface Method Descriptions getSupportedProperties : Returns a range of StorageProperty instances that are valid for the current StorageService implementation. All methods below throw an error if a non-supported property is passed to them. isPropertySet : Determines whether there currently is a value associated with prop . getProperty : Returns the value currently associated with prop , if available. If the optional type parameter is present, the result of this method is of type V (the type of prop ), otherwise Object . setProperty : Associates prop with the value `value and returns the value it previously was associated with, if available. unsetProperty : Removes the association of prop with a value and returns the value it previously was associated with, if available. Example: CsvStorageConfig Implementation (Prose) The implementation of all interface methods (except getSupportedProperties ) is very similar for (almost) all storage mechanisms. Therefore, it is recommended to provide an abstract base class that conforms to the informal descriptions above. That way, concrete StorageConfig implementation need only be concerned with supplying the supported storage properties. The kinds of properties conceivable for a CsvStorageConfig are enumerated on the tab concerned with the StorageProperty interface. Represents a configuration property to be used within StorageConfig implementations. Instances of this interface are not associated with values. The actual values of storage configuration properties are managed solely by StorageConfig instances. Furthermore, object equality is determined by their identifier. A StorageConfig must not support two different properties with the same identifier. Example: CsvStorageProperty Implementation (Prose) Depending on the concrete programming language, the way of implementing multiple storage properties can differ widely in both manner and succinctity (lambdas, anonymous classes, constants, static final instances, parameterized enum members). The table below summarizes some possible configuration properties of a CSV storage service. name identifier type used by FILE_PATH filePath String load , store FIELD_SEPARATOR fieldSeparator Character load , store TIME_COLUMN timeColumn Integer transform TIME_FORMAT timeFormat String transform , store VALUE_COLUMN valueColumn Integer transform SKIP_HEADERS skipHeaders Integer load APPEND append Boolean store This second example demonstrates a succinct Java representation of the table above, in the form of an enum implementing the StorageProperty interface. Example: CsvStorageProperty Implementation (Java) public enum CsvStorageProperty implements StorageProperty { /** * Used by load, store. */ FILE_PATH ( \"filePath\" , String . class ), /** * Used by load, store. */ FIELD_SEPARATOR ( \"fieldSeparator\" , Character . class ), /** * Used by transform. */ TIME_COLUMN ( \"timeColumn\" , Integer . class ), /** * Used by transform, store. */ TIME_FORMAT ( \"timeFormat\" , String . class ), /** * Used by transform. */ VALUE_COLUMN ( \"valueColumn\" , Integer . class ), /** * Used by load. */ SKIP_HEADERS ( \"skipHeaders\" , Integer . class ), /* * Used by store. */ APPEND ( \"append\" , Boolean . class ), private final String identifier ; private final Class <?> type ; CsvStorageProperty ( String identifier , Class <?> type ) { this . identifier = identifier ; this . type = type ; } @Override public String identifier () { return identifier ; } @Override public Class <?> type () { return type ; } }","title":"Query API Overview"},{"location":"query-api/query-api-overview/#query-api","text":"In order to ensure the generality of DTSQL, it operates on a canonical representation of time series (as already explained in the respective section about Time Series ). This allows developers to hook up any storage system containing time series data (CSV files, InfluxDB, SQL databases, web services, \u2026) to DTSQL. The paragraphs below explain both how canonical time series are queried and which interfaces are involved in obtaining such queryable time series.","title":"Query API"},{"location":"query-api/query-api-overview/#query-service","text":"The QueryService interface is very simple, providing only a single method that takes a time series and a DTSQL query and returns a query result (see also the section about Query Results ). Implementations should parse the query into an appropriate internal representation and then evaluate it based on the respective input time series. classDiagram class QueryService { <<interface>> +query(ts: TimeSeries, dtsqlQuery: String) QueryResult }","title":"Query Service"},{"location":"query-api/query-api-overview/#storage-services","text":"Before feeding a canonical time series to a QueryService implementation, it has to be extracted from an arbitrary data source. This is possible via the StorageService , StorageConfig and StorageConfig interfaces. In general, StorageService provides data access, parameterized by a StorageConfig instance consisting of several StorageProperty values. The diagrams below visualize the members of each interface. Underneath them, dedicated content tabs provide a more in-depth view of these three interfaces, explaining their individual methods and depicting succinct examples of how they may be utilized in practice. classDiagram class StorageService~T, U extends StorageConfig~ { <<interface>> +initialize(svcCfg: U) +isInitialized() Boolean +load(lookupCfg: U) T[] +transform(loadedData: T[], transformCfg: U) TimeSeries +store(data: TimeSeries, persistCfg: U) } class StorageProperty { <<interface>> +getIdentifier() String +getType() Class~V~ } classDiagram class StorageConfig { <<interface>> +getSupportedProperties() StorageProperty[] +isPropertySet(prop: StorageProperty) Boolean +getProperty(prop: StorageProperty, [type: Class~V~]) V +setProperty(prop: StorageProperty, value: Object) Object +unsetProperty(prop: StorageProperty) Object } StorageService StorageConfig StorageProperty Represents an abstraction to be used to connect a custom storage mechanism to a Query Service instance. Info: Interface Method Descriptions T : The storage-specific (native) type of data retrieved by load . U : The configuration compatible with current StorageService implementation, must be a subtype of StorageConfig . initialize : Can be used for general setup specific to the storage (e.g., connect to database, check I/O availability, \u2026). isInitialized : Determines whether the service has been initialized; is a precondition for store , load and transform . load : Reads required data from the storage with parameters specified by lookupCfg into the native data type T . transform : Converts the data obtained from load into a TimeSeries instance according to the parameters represented by transformCfg . store : Is meant to store/persist a TimeSeries instance into the storage system. This operation basically conducts the inverse conversion of transform . This method is not needed in the majority of cases. Therefore, generally, it is valid not to implement this method. Example: CsvStorageService Implementation (Prose) For this example, we assume CSV file accesses are encapsulated by an third-party library. T : CsvRow , a data structure provided by the library that represents a line in a CSV file. U : CsvStorageConfig , a derived type of StorageConfig that is valid to feed to CsvStorageService (see second tab page for details). initialize : Since file streams are opened and closed at the moment of accessing CSV files, there is nothing like an underlying database connection or web session. Therefore, this method may be empty. isInitialized : For the reasons explained above, this method may always return true . load : The CsvStorageConfig instance lookupCfg contains parameters such as file path, field delimiter, and a number of lines to skip before parsing. Using those parameters, the CSV library can be instructed to load the desiredd data as CsvRow instance. transform : In order to convert a range of CsvRow instances to DataPoint objects (and subsequently a TimeSeries ), the transformCfg contains information such as index of date and value columns and date/time format to correctly parse time components of data points. store : If one were to implement this method, persistCfg should contain information such as destination path, field separators to use, format to serialize time components into and possibly an option to toggle create/append file modes. Represents a container for the configuration (properties) of a concrete StorageService . Info: Interface Method Descriptions getSupportedProperties : Returns a range of StorageProperty instances that are valid for the current StorageService implementation. All methods below throw an error if a non-supported property is passed to them. isPropertySet : Determines whether there currently is a value associated with prop . getProperty : Returns the value currently associated with prop , if available. If the optional type parameter is present, the result of this method is of type V (the type of prop ), otherwise Object . setProperty : Associates prop with the value `value and returns the value it previously was associated with, if available. unsetProperty : Removes the association of prop with a value and returns the value it previously was associated with, if available. Example: CsvStorageConfig Implementation (Prose) The implementation of all interface methods (except getSupportedProperties ) is very similar for (almost) all storage mechanisms. Therefore, it is recommended to provide an abstract base class that conforms to the informal descriptions above. That way, concrete StorageConfig implementation need only be concerned with supplying the supported storage properties. The kinds of properties conceivable for a CsvStorageConfig are enumerated on the tab concerned with the StorageProperty interface. Represents a configuration property to be used within StorageConfig implementations. Instances of this interface are not associated with values. The actual values of storage configuration properties are managed solely by StorageConfig instances. Furthermore, object equality is determined by their identifier. A StorageConfig must not support two different properties with the same identifier. Example: CsvStorageProperty Implementation (Prose) Depending on the concrete programming language, the way of implementing multiple storage properties can differ widely in both manner and succinctity (lambdas, anonymous classes, constants, static final instances, parameterized enum members). The table below summarizes some possible configuration properties of a CSV storage service. name identifier type used by FILE_PATH filePath String load , store FIELD_SEPARATOR fieldSeparator Character load , store TIME_COLUMN timeColumn Integer transform TIME_FORMAT timeFormat String transform , store VALUE_COLUMN valueColumn Integer transform SKIP_HEADERS skipHeaders Integer load APPEND append Boolean store This second example demonstrates a succinct Java representation of the table above, in the form of an enum implementing the StorageProperty interface. Example: CsvStorageProperty Implementation (Java) public enum CsvStorageProperty implements StorageProperty { /** * Used by load, store. */ FILE_PATH ( \"filePath\" , String . class ), /** * Used by load, store. */ FIELD_SEPARATOR ( \"fieldSeparator\" , Character . class ), /** * Used by transform. */ TIME_COLUMN ( \"timeColumn\" , Integer . class ), /** * Used by transform, store. */ TIME_FORMAT ( \"timeFormat\" , String . class ), /** * Used by transform. */ VALUE_COLUMN ( \"valueColumn\" , Integer . class ), /** * Used by load. */ SKIP_HEADERS ( \"skipHeaders\" , Integer . class ), /* * Used by store. */ APPEND ( \"append\" , Boolean . class ), private final String identifier ; private final Class <?> type ; CsvStorageProperty ( String identifier , Class <?> type ) { this . identifier = identifier ; this . type = type ; } @Override public String identifier () { return identifier ; } @Override public Class <?> type () { return type ; } }","title":"Storage Services"},{"location":"query-api/storage-mechanisms/csv/","text":"CSV Storage Service # This StorageService implementation is responsible for accessing (locally stored) CSV files. Its behaviour is in accordance with the interface contract. The supported parameterization, i.e., the configuration properties, are defined below. Tip The tables may be sorted by clicking on the respective column header. Initialize Configuration Properties # At the moment, this service does not have a special initialization routine. Identifier Type Optional? Constraint(s) Description Load Configuration Properties # Identifier Type Optional? Constraint(s) Description filePath String The path of the local file to read the time series from. fieldSeparator Character The character that separates fields (columns) in the file. skipHeaders Integer Must not be less than 0 . The number of rows to skip before the data to actually load starts. customEndOfFileMarkers String[] If present, instructs the CSV parser to abort if a line whose content is equivalent to one of the custom EOF markers is encountered. This is useful for when you know a CSV file contains additional information beyond a certain point which is not relevant in that context. Transform Configuration Properties # Identifier Type Optional? Constraint(s) Description valueColumn Integer Must be less than the number of columns in the file and not less than 0 . The index of the column which contains the value component of data points in the time series to load. timeColumn Integer Must be less than the number of columns in the file and not less than 0 . The index of the column which contains the time component of data points in the time series to load. timeFormat String Must be a valid (Java) date-time pattern. The date-time pattern (format) to be used when parsing the time component of a data point. Store Configuration Properties # Identifier Type Optional? Constraint(s) Description filePath String The path of the file the time series should be serialized to. fieldSeparator Character The character to use to separate fields (columns) in the file. timeFormat String Must be a valid (Java) date-time pattern. The date-time pattern (format) to use when serializing the time component of a data point. append Boolean If true , the serialized time series is appended to the file specified by filePath . Otherwise, it (re-)creates the file, potentially truncating pre-existing contents. includeHeaders Boolean Specifies whether CSV headers should be included during time series serialization. timeColumnLabel String If includeHeaders is true , this property is mandatory. The header of the column containing time components of the data points of the serialized time series. valueColumnLabel String If includeHeaders is true , this property is mandatory. The header of the column containing value components of the data points of the serialized time series.","title":"CSV Storage Service"},{"location":"query-api/storage-mechanisms/csv/#csv-storage-service","text":"This StorageService implementation is responsible for accessing (locally stored) CSV files. Its behaviour is in accordance with the interface contract. The supported parameterization, i.e., the configuration properties, are defined below. Tip The tables may be sorted by clicking on the respective column header.","title":"CSV Storage Service"},{"location":"query-api/storage-mechanisms/csv/#initialize-configuration-properties","text":"At the moment, this service does not have a special initialization routine. Identifier Type Optional? Constraint(s) Description","title":"Initialize Configuration Properties"},{"location":"query-api/storage-mechanisms/csv/#load-configuration-properties","text":"Identifier Type Optional? Constraint(s) Description filePath String The path of the local file to read the time series from. fieldSeparator Character The character that separates fields (columns) in the file. skipHeaders Integer Must not be less than 0 . The number of rows to skip before the data to actually load starts. customEndOfFileMarkers String[] If present, instructs the CSV parser to abort if a line whose content is equivalent to one of the custom EOF markers is encountered. This is useful for when you know a CSV file contains additional information beyond a certain point which is not relevant in that context.","title":"Load Configuration Properties"},{"location":"query-api/storage-mechanisms/csv/#transform-configuration-properties","text":"Identifier Type Optional? Constraint(s) Description valueColumn Integer Must be less than the number of columns in the file and not less than 0 . The index of the column which contains the value component of data points in the time series to load. timeColumn Integer Must be less than the number of columns in the file and not less than 0 . The index of the column which contains the time component of data points in the time series to load. timeFormat String Must be a valid (Java) date-time pattern. The date-time pattern (format) to be used when parsing the time component of a data point.","title":"Transform Configuration Properties"},{"location":"query-api/storage-mechanisms/csv/#store-configuration-properties","text":"Identifier Type Optional? Constraint(s) Description filePath String The path of the file the time series should be serialized to. fieldSeparator Character The character to use to separate fields (columns) in the file. timeFormat String Must be a valid (Java) date-time pattern. The date-time pattern (format) to use when serializing the time component of a data point. append Boolean If true , the serialized time series is appended to the file specified by filePath . Otherwise, it (re-)creates the file, potentially truncating pre-existing contents. includeHeaders Boolean Specifies whether CSV headers should be included during time series serialization. timeColumnLabel String If includeHeaders is true , this property is mandatory. The header of the column containing time components of the data points of the serialized time series. valueColumnLabel String If includeHeaders is true , this property is mandatory. The header of the column containing value components of the data points of the serialized time series.","title":"Store Configuration Properties"},{"location":"query-api/storage-mechanisms/influxdb/","text":"CSV Storage Service # This StorageService implementation is responsible for accessing data stored in an InfluxDB database. Its behaviour is in accordance with the interface contract. The supported parameterization, i.e., the configuration properties, are defined below. Tip The tables may be sorted by clicking on the respective column header. Initialize Configuration Properties # Identifier Type Optional? Constraint(s) Description url String The URL the InfluxDB from which a time series should be extracted is accessible at. token Character[] The access token required to establish a connection to the InfluxDB instance. organization String The InfluxDB organization containing the time series to extract. bucket String The bucket of the InfluxDB instance containing the time series to extract. Load Configuration Properties # Identifier Type Optional? Constraint(s) Description query String Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the InfluxDB query which is used to extract the time series from the database instance. bucket Character Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the name of the InfluxDB bucket containing the time series to extract. loadFrom Instant (Date/Time) Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the temporal lower bound from which data points contained in the bucket specified by bucket should be extracted. loadUntil Instant (Date(Time) Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the temporal upper bound until which data points contained in the bucket specified by bucket should be extracted. Transform Configuration Properties # Identifier Type Optional? Constraint(s) Description tableIndex Integer Must be equal to or greater than -1 (but less than the number of FluxTable instances returned by load ). The index of the FluxTable returned by load whose data points should be used to construct the canonical time series. If the value of equal to -1 , the data points from all tables are used. Otherwise, i.e., for values equal to or greater than 0 , only the data points from the table with this very index are used. Store Configuration Properties # Bug The store operation has not been implemented yet. Identifier Type Optional? Constraint(s) Description","title":"InfluxDB Storage Service"},{"location":"query-api/storage-mechanisms/influxdb/#csv-storage-service","text":"This StorageService implementation is responsible for accessing data stored in an InfluxDB database. Its behaviour is in accordance with the interface contract. The supported parameterization, i.e., the configuration properties, are defined below. Tip The tables may be sorted by clicking on the respective column header.","title":"CSV Storage Service"},{"location":"query-api/storage-mechanisms/influxdb/#initialize-configuration-properties","text":"Identifier Type Optional? Constraint(s) Description url String The URL the InfluxDB from which a time series should be extracted is accessible at. token Character[] The access token required to establish a connection to the InfluxDB instance. organization String The InfluxDB organization containing the time series to extract. bucket String The bucket of the InfluxDB instance containing the time series to extract.","title":"Initialize Configuration Properties"},{"location":"query-api/storage-mechanisms/influxdb/#load-configuration-properties","text":"Identifier Type Optional? Constraint(s) Description query String Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the InfluxDB query which is used to extract the time series from the database instance. bucket Character Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the name of the InfluxDB bucket containing the time series to extract. loadFrom Instant (Date/Time) Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the temporal lower bound from which data points contained in the bucket specified by bucket should be extracted. loadUntil Instant (Date(Time) Either only query or (exclusively) bucket , loadFrom , loadUntil must be present. If present, the temporal upper bound until which data points contained in the bucket specified by bucket should be extracted.","title":"Load Configuration Properties"},{"location":"query-api/storage-mechanisms/influxdb/#transform-configuration-properties","text":"Identifier Type Optional? Constraint(s) Description tableIndex Integer Must be equal to or greater than -1 (but less than the number of FluxTable instances returned by load ). The index of the FluxTable returned by load whose data points should be used to construct the canonical time series. If the value of equal to -1 , the data points from all tables are used. Otherwise, i.e., for values equal to or greater than 0 , only the data points from the table with this very index are used.","title":"Transform Configuration Properties"},{"location":"query-api/storage-mechanisms/influxdb/#store-configuration-properties","text":"Bug The store operation has not been implemented yet. Identifier Type Optional? Constraint(s) Description","title":"Store Configuration Properties"},{"location":"query-language/events/","text":"Events # An event is a declarative specification of intervals in which some condition(s) hold(s). In other words, they are used to detect periods where areas of interest occur. Events are evaluated after filter application. In fact, they extend the notion of filters by considering detected intervals as continuous (i.e., without interruption) (sub-)series of data points for which a propositional formula composed of event function instances is satisfied. The difference to filters is that, with events, data points are not examined individually anymore, but they are grouped together to form periods satisfying the criteria imposed by the formula. Abstract: Types of DTSQL Events Filter Events : They are syntactically and semantically equivalent to filter functions with respect to the filtered time series. Complex Events : They are broader and, typically, conceptually more involved and may capture any type of occurrence in a time series as long as it can be specified formally. There is one key commonality among all kinds of event functions. Generally, DTSQL is only interested in maximal interval , i.e., ones that cannot be extended without violating the event function. For filter events, this means that extensions of the respective interval by exactly one data point to the left and right, respectively, are examined (looking further than that could produce intervals with gaps, which are not considered). For complex events, the situation is more difficult: It may be possible to extend an interval in either direction (temporarily violating the event function) and extending it even more to, in the end, satisfy it again. In other words, extending a valid locally maximal interval might yield a valid globally maximal interval despite potentially invalidating it temporarily. In order to compose a propositional formula of event function instances, the following keywords are available: conjunction: AND (n-ary) disjunction: OR (n-ary) negation: NOT (unary) Warning: Syntactic Restrictions of Event Formulas At the moment, DTSQL does not support nested conjunctions and disjunctions. This means, formulas like AND(a, b, NOT(c)) or OR(NOT(a), NOT(b), c) are expressible, but ones like AND(AND(a), b) or NOT(AND(a, b), OR(NOT(c), d)) are not. More specifically, root elements of a formula must be either AND or OR and may only contain arbitrarily many positive or negated (using NOT ) filter function instances. Bug: Shortcoming of Reference Implementation Regarding Event Formulas The current DTSQL reference implementation is not yet able to combine a complex event function instance with other event function instances. In other words, if an event formula contains a complex event function instance, it must not contain any other event function instance. Mixing and combining filter event functions is supported the same way it is for filter definitions , though. In summary, the syntax of DTSQL\u2019s events component is: ['USING EVENTS:' {'AND('|'OR('}{'NOT('<eventFunction>')'|<eventFunction>}...')' ['FOR' {'('|'['} [<minLength>]',' [<maxLength>] {')'|']'} <unit>] 'AS' <eventIdentifier>...] Again, the component is optional. However, should it be present, then at least one event function instance must be declared. The <eventFunction> placeholder is to be replaced by event function instances as explained below (with commas , as separator). Note that, if only one event function instance is used, then the choice of AND vs. OR does not make a difference. Unlike the filter component, the events component may declare multiple events (with commas , as separator) and assign identifiers to them such that they may be reused in the selection component . Furthermore, an event may be equipped with a duration constraint: <minLength> and <maxLength> are optional integers specifying the required length of the event (leaving them out represents 0 and \u201cinfinity\u201d, respectively). The inclusiveness or exclusiveness of the minimum and maximum is determined based on the parenthesis used: lower bound upper bound inclusive [ ] exclusive ( ) Finally, <unit> should be replaced by a supported time unit . Event Functions # Filter Events # lt({<threshold>|<sampleIdentifier>}) Represents maximal intervals where the value component is less than the threshold (which may refer to a sample). gt({<threshold>|<sampleIdentifier>}) Represents maximal intervals where the value component is greater than the threshold (which may refer to a sample). around(abs, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Represents maximal intervals where the absolute difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). around(rel, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Represents maximal intervals where if the relative (percentage-wise) difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). The reference value must not be equal to zero. before(<threshold>) Represents maximal intervals where the time component is before the temporal threshold. after(<threshold>) Represents maximal intervals where the time component is after the temporal threshold. Complex Events # const({<slope>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Detects maximal intervals with (approximately) constant values. This means the absolute value of the slope of the regression line in such an interval is not greater than the specified slope (which may refer to a sample) in percent (must not be less than 0). Furthermore, the values of the interval may not deviate more than the specified deviation (which may refer to a sample) in percent from the interval\u2019s average (must not be less than 0). increase({<minChange>|<sampleIdentifier>}, {<maxChange>|<sampleIdentifier>}, {<tolerance>|<sampleIdentifier>}) Detects maximal intervals depicting (approximately) monotonic increases. The change in value captured by the interval must be non-negative and between the specified minimum and maximum change (which both may refer to a sample and must not be less than 0; \u201cinfinity\u201d upper bound can be expressed using - ). Temporary decreases in value are tolerated as long as their respective instantaneous rate of change does not fall below -t percent, where t represents the specified tolerance (which may refer to a sample). decrease({<minChange>|<sampleIdentifier>}, {<maxChange>|<sampleIdentifier>}, {<tolerance>|<sampleIdentifier>}) Detects maximal intervals depicting (approximately) monotonic decreases. The change in value captured by the interval must be non-positive and between the specified minimum and maximum change (which both may refer to a sample and must not be less than 0; \u201cinfinity\u201d upper bound can be expressed using - ). Temporary increases in value are tolerated as long as their respective instantaneous rate of change does not exceed +t percent, where t represents the specified tolerance (which may refer to a sample). Examples # Example: Events Component (1) The events component below should detect two kinds of incidents: Periods in which the recorded value is either above 300 or below 100 for at least 30 minutes. Periods of rapid, steep monotonic increases of at least 50 % in the space of 45 seconds with a tolerance against decreases of 5.75 %. Also, such periods should not appear before January 10th 2022. USING EVENTS: OR(gt(300), lt(100)) FOR [30,] minutes AS e1, AND( increase(50, -, 5.75), NOT(before(\"2022-01-10T00:00:00Z\")) ) FOR [,45] seconds AS e2 Example: Events Component (2) The events component below should detect two kinds of incidents: Periods in which the value is above the global average. Periods in which the value is below or equal to the global average. WITH SAMPLES: avg() AS avgGlobal USING EVENTS: AND(gt(avgGlobal)) AS e1, AND(NOT(gt(avgGlobal))) AS e2","title":"Events"},{"location":"query-language/events/#events","text":"An event is a declarative specification of intervals in which some condition(s) hold(s). In other words, they are used to detect periods where areas of interest occur. Events are evaluated after filter application. In fact, they extend the notion of filters by considering detected intervals as continuous (i.e., without interruption) (sub-)series of data points for which a propositional formula composed of event function instances is satisfied. The difference to filters is that, with events, data points are not examined individually anymore, but they are grouped together to form periods satisfying the criteria imposed by the formula. Abstract: Types of DTSQL Events Filter Events : They are syntactically and semantically equivalent to filter functions with respect to the filtered time series. Complex Events : They are broader and, typically, conceptually more involved and may capture any type of occurrence in a time series as long as it can be specified formally. There is one key commonality among all kinds of event functions. Generally, DTSQL is only interested in maximal interval , i.e., ones that cannot be extended without violating the event function. For filter events, this means that extensions of the respective interval by exactly one data point to the left and right, respectively, are examined (looking further than that could produce intervals with gaps, which are not considered). For complex events, the situation is more difficult: It may be possible to extend an interval in either direction (temporarily violating the event function) and extending it even more to, in the end, satisfy it again. In other words, extending a valid locally maximal interval might yield a valid globally maximal interval despite potentially invalidating it temporarily. In order to compose a propositional formula of event function instances, the following keywords are available: conjunction: AND (n-ary) disjunction: OR (n-ary) negation: NOT (unary) Warning: Syntactic Restrictions of Event Formulas At the moment, DTSQL does not support nested conjunctions and disjunctions. This means, formulas like AND(a, b, NOT(c)) or OR(NOT(a), NOT(b), c) are expressible, but ones like AND(AND(a), b) or NOT(AND(a, b), OR(NOT(c), d)) are not. More specifically, root elements of a formula must be either AND or OR and may only contain arbitrarily many positive or negated (using NOT ) filter function instances. Bug: Shortcoming of Reference Implementation Regarding Event Formulas The current DTSQL reference implementation is not yet able to combine a complex event function instance with other event function instances. In other words, if an event formula contains a complex event function instance, it must not contain any other event function instance. Mixing and combining filter event functions is supported the same way it is for filter definitions , though. In summary, the syntax of DTSQL\u2019s events component is: ['USING EVENTS:' {'AND('|'OR('}{'NOT('<eventFunction>')'|<eventFunction>}...')' ['FOR' {'('|'['} [<minLength>]',' [<maxLength>] {')'|']'} <unit>] 'AS' <eventIdentifier>...] Again, the component is optional. However, should it be present, then at least one event function instance must be declared. The <eventFunction> placeholder is to be replaced by event function instances as explained below (with commas , as separator). Note that, if only one event function instance is used, then the choice of AND vs. OR does not make a difference. Unlike the filter component, the events component may declare multiple events (with commas , as separator) and assign identifiers to them such that they may be reused in the selection component . Furthermore, an event may be equipped with a duration constraint: <minLength> and <maxLength> are optional integers specifying the required length of the event (leaving them out represents 0 and \u201cinfinity\u201d, respectively). The inclusiveness or exclusiveness of the minimum and maximum is determined based on the parenthesis used: lower bound upper bound inclusive [ ] exclusive ( ) Finally, <unit> should be replaced by a supported time unit .","title":"Events"},{"location":"query-language/events/#event-functions","text":"","title":"Event Functions"},{"location":"query-language/events/#filter-events","text":"lt({<threshold>|<sampleIdentifier>}) Represents maximal intervals where the value component is less than the threshold (which may refer to a sample). gt({<threshold>|<sampleIdentifier>}) Represents maximal intervals where the value component is greater than the threshold (which may refer to a sample). around(abs, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Represents maximal intervals where the absolute difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). around(rel, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Represents maximal intervals where if the relative (percentage-wise) difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). The reference value must not be equal to zero. before(<threshold>) Represents maximal intervals where the time component is before the temporal threshold. after(<threshold>) Represents maximal intervals where the time component is after the temporal threshold.","title":"Filter Events"},{"location":"query-language/events/#complex-events","text":"const({<slope>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Detects maximal intervals with (approximately) constant values. This means the absolute value of the slope of the regression line in such an interval is not greater than the specified slope (which may refer to a sample) in percent (must not be less than 0). Furthermore, the values of the interval may not deviate more than the specified deviation (which may refer to a sample) in percent from the interval\u2019s average (must not be less than 0). increase({<minChange>|<sampleIdentifier>}, {<maxChange>|<sampleIdentifier>}, {<tolerance>|<sampleIdentifier>}) Detects maximal intervals depicting (approximately) monotonic increases. The change in value captured by the interval must be non-negative and between the specified minimum and maximum change (which both may refer to a sample and must not be less than 0; \u201cinfinity\u201d upper bound can be expressed using - ). Temporary decreases in value are tolerated as long as their respective instantaneous rate of change does not fall below -t percent, where t represents the specified tolerance (which may refer to a sample). decrease({<minChange>|<sampleIdentifier>}, {<maxChange>|<sampleIdentifier>}, {<tolerance>|<sampleIdentifier>}) Detects maximal intervals depicting (approximately) monotonic decreases. The change in value captured by the interval must be non-positive and between the specified minimum and maximum change (which both may refer to a sample and must not be less than 0; \u201cinfinity\u201d upper bound can be expressed using - ). Temporary increases in value are tolerated as long as their respective instantaneous rate of change does not exceed +t percent, where t represents the specified tolerance (which may refer to a sample).","title":"Complex Events"},{"location":"query-language/events/#examples","text":"Example: Events Component (1) The events component below should detect two kinds of incidents: Periods in which the recorded value is either above 300 or below 100 for at least 30 minutes. Periods of rapid, steep monotonic increases of at least 50 % in the space of 45 seconds with a tolerance against decreases of 5.75 %. Also, such periods should not appear before January 10th 2022. USING EVENTS: OR(gt(300), lt(100)) FOR [30,] minutes AS e1, AND( increase(50, -, 5.75), NOT(before(\"2022-01-10T00:00:00Z\")) ) FOR [,45] seconds AS e2 Example: Events Component (2) The events component below should detect two kinds of incidents: Periods in which the value is above the global average. Periods in which the value is below or equal to the global average. WITH SAMPLES: avg() AS avgGlobal USING EVENTS: AND(gt(avgGlobal)) AS e1, AND(NOT(gt(avgGlobal))) AS e2","title":"Examples"},{"location":"query-language/filters/","text":"Filters # A filter enables query creators to exclude data points of an input time series from the query evaluation. They are applied after the computation of samples (otherwise they could not be referenced in filter definitions). DTSQL filters are conceptually simple in that they do not apply complex transformations as, e.g., a low-pass filter does. They merely decide whether a data point should be included in the query evaluation, solely based on the information provided by the respective data point itself. A DTSQL query contains zero to one filter specifications. The enumeration below summarizes how filters work in general. A filter specification is a connection of filter functions in a propositional logical manner. Abstract: Principles of DTSQL Filters A filter specification consists of multiple filter function applications. A filter function is always evaluated based on a single data point. Filter functions are connected to each other in a propositional logical manner. A data point is included in the query evaluation if and only if the thus resulting propositional formula is satisfied given its time and value component. In order to compose a propositional formula of filter function instances, the following keywords are available: conjunction: AND (n-ary) disjunction: OR (n-ary) negation: NOT (unary) Warning: Syntactic Restrictions of Filter Formulas At the moment, DTSQL does not support nested conjunctions and disjunctions. This means, formulas like AND(a, b, NOT(c)) or OR(NOT(a), NOT(b), c) are expressible, but ones like AND(AND(a), b) or NOT(AND(a, b), OR(NOT(c), d)) are not. More specifically, root elements of a formula must be either AND or OR and may only contain arbitrarily many positive or negated (using NOT ) filter function instances. As a result of this, the syntax of DTSQL\u2019s filter component is: ['APPLY FILTER:' {'AND('|'OR('}{'NOT('<filterFunction>')'|<filterFunction>}...')'] Again, the component is optional. However, should it be present, then at least one filter function instance must be declared. The <filterFunction> placeholder is to be replaced by filter function instances as explained below (with commas , as separator). Note that, if only one filter function instance is used, then the choice of AND vs. OR does not make a difference. Filter Functions # Threshold Filters # lt({<threshold>|<sampleIdentifier>}) Is satisfied if the value component is less than the threshold (which may refer to a sample). gt({<threshold>|<sampleIdentifier>}) Is satisfied if the value component is greater than the threshold (which may refer to a sample). Deviation Filters # around(abs, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Is satisfied if the absolute difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). around(rel, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Is satisfied if the relative (percentage-wise) difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). The reference value must not be equal to zero. Temporal Filters # before(<threshold>) Is satisfied if the time component is before the temporal threshold. after(<threshold>) Is satisfied if the time component is after the temporal threshold. Example # The query below illustrates one of the three types of filter functions from above (all three conditions should be fulfilled): Only keep data points starting with June 24th 2021 13:24 UTC. Filter out data points which are equal to or less than 240. Only keep data points which are in a \u00b15.25 % range of the global average. Example: Filter Component WITH SAMPLES: avg() AS globalAvg APPLY FILTER: AND( NOT(before(\"2021-06-24T13:24:00Z\")), gt(240), around(rel, globalAvg, 5.25) )","title":"Filters"},{"location":"query-language/filters/#filters","text":"A filter enables query creators to exclude data points of an input time series from the query evaluation. They are applied after the computation of samples (otherwise they could not be referenced in filter definitions). DTSQL filters are conceptually simple in that they do not apply complex transformations as, e.g., a low-pass filter does. They merely decide whether a data point should be included in the query evaluation, solely based on the information provided by the respective data point itself. A DTSQL query contains zero to one filter specifications. The enumeration below summarizes how filters work in general. A filter specification is a connection of filter functions in a propositional logical manner. Abstract: Principles of DTSQL Filters A filter specification consists of multiple filter function applications. A filter function is always evaluated based on a single data point. Filter functions are connected to each other in a propositional logical manner. A data point is included in the query evaluation if and only if the thus resulting propositional formula is satisfied given its time and value component. In order to compose a propositional formula of filter function instances, the following keywords are available: conjunction: AND (n-ary) disjunction: OR (n-ary) negation: NOT (unary) Warning: Syntactic Restrictions of Filter Formulas At the moment, DTSQL does not support nested conjunctions and disjunctions. This means, formulas like AND(a, b, NOT(c)) or OR(NOT(a), NOT(b), c) are expressible, but ones like AND(AND(a), b) or NOT(AND(a, b), OR(NOT(c), d)) are not. More specifically, root elements of a formula must be either AND or OR and may only contain arbitrarily many positive or negated (using NOT ) filter function instances. As a result of this, the syntax of DTSQL\u2019s filter component is: ['APPLY FILTER:' {'AND('|'OR('}{'NOT('<filterFunction>')'|<filterFunction>}...')'] Again, the component is optional. However, should it be present, then at least one filter function instance must be declared. The <filterFunction> placeholder is to be replaced by filter function instances as explained below (with commas , as separator). Note that, if only one filter function instance is used, then the choice of AND vs. OR does not make a difference.","title":"Filters"},{"location":"query-language/filters/#filter-functions","text":"","title":"Filter Functions"},{"location":"query-language/filters/#threshold-filters","text":"lt({<threshold>|<sampleIdentifier>}) Is satisfied if the value component is less than the threshold (which may refer to a sample). gt({<threshold>|<sampleIdentifier>}) Is satisfied if the value component is greater than the threshold (which may refer to a sample).","title":"Threshold Filters"},{"location":"query-language/filters/#deviation-filters","text":"around(abs, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Is satisfied if the absolute difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). around(rel, {<reference>|<sampleIdentifier>}, {<deviation>|<sampleIdentifier>}) Is satisfied if the relative (percentage-wise) difference between the value component and the reference value (which may refer to a sample) is less than the maximum deviation (which may refer to a sample). The reference value must not be equal to zero.","title":"Deviation Filters"},{"location":"query-language/filters/#temporal-filters","text":"before(<threshold>) Is satisfied if the time component is before the temporal threshold. after(<threshold>) Is satisfied if the time component is after the temporal threshold.","title":"Temporal Filters"},{"location":"query-language/filters/#example","text":"The query below illustrates one of the three types of filter functions from above (all three conditions should be fulfilled): Only keep data points starting with June 24th 2021 13:24 UTC. Filter out data points which are equal to or less than 240. Only keep data points which are in a \u00b15.25 % range of the global average. Example: Filter Component WITH SAMPLES: avg() AS globalAvg APPLY FILTER: AND( NOT(before(\"2021-06-24T13:24:00Z\")), gt(240), around(rel, globalAvg, 5.25) )","title":"Example"},{"location":"query-language/overview/","text":"DTSQL Query Overview # Notational Conventions # This documentation follows (and goes beyond) common conventions known from usage messages (program synopsis), e.g., in UNIX man pages. They are summarized below. Abstract: Notational Conventions for Syntax Definitions Optional arguments [a] : a is optional. Alternative options [a|b] : May be left out, but if present, choose either a or b . Alternative required options {a|b} : Must not be left out; choose either a or b . Conditionally valid options [-a arg [-b]] : If -a arg is present, then optionally adding -b is also valid. Multiple occurrence a... : A range of a occurrences is allowed. The context must specify both whether cardinalities 0..n or 1..n are valid as well as the separator to use (e.g., whitespace or comma). Placeholders <a> : The placeholder <a> represents a concept that is explained later on or below. Lexicographic elements 'a' : Single-quoted text like 'a' stands for lexicographic elements to be used verbatim in a query (to distinguish them from syntax definition symbols). Whitespaces are implicitly understood as such. Query Syntax # The Getting Started page has already provided an exemplary query. The general structure of DTSQL queries is as follows: ['WITH SAMPLES:' {<GlobalValueAggregate>|<LocalValueAggregate>|<TemporalAggregate>} 'AS' <sampleIdentifier>...] ['APPLY FILTER:' {'AND('|'OR('}{'NOT('<filterFunction>')'|<filterFunction>}...')'] ['USING EVENTS:' {'AND('|'OR('}{'NOT('<eventFunction>')'|<eventFunction>}...')' ['FOR' {'('|'['} [<minLength>]',' [<maxLength>] {')'|']'} <unit>] 'AS' <eventIdentifier>...] ['SELECT PERIODS:' <selection>] 'YIELD:' {'all periods'|'longest period'|'shortest period'|'data points'|'sample' <sampleIdentifier>|'samples' <sampleIdentifier>...} As already mentioned before, all components (apart from YIELD ) are optional. Furthermore, whitespaces do not carry semantics. In other words, a DTSQL query may be formatted arbitrarily. There are instances, however, that require a whitespace (space, new line, tab), e.g., after the colon ( : ) at the beginning of each component. Examples # In order to illustrate the syntax depicted above, the following paragraphs feature some (simple and not practice-oriented) exemplary queries along with their purpose. The next pages will explain how the elements of the components mentioned above, and contained in the sample queries, are defined exactly. Example: DTSQL Query (1) Capture and return global, local and temporal aggregates: WITH SAMPLES: avg() AS globalAverage, integral(\"2017-09-09T10:00:00Z\", \"\") AS integralTillTheEnd, sum_t(days, \"2017-09-09T10:00:00Z/2019-09-04T10:00:00Z\", \"2018-06-11T10:00:00Z/2019-01-08T10:00:00Z\", \"2019-06-27T10:00:00Z/2019-09-12T10:00:00Z\") AS totalDuration YIELD: samples globalAverage, integralTillTheEnd, totalDuration Example: DTSQL Query (2) Return data points after a temporal threshold which are greater than the global average and within a \u00b110 % range of the global maximum. WITH SAMPLES: avg() AS globalAvg, max() AS globalMax APPLY FILTER: AND(after(\"2018-11-12T07:12:59+03:30\"), gt(globalAvg), around(rel, globalMax, 10)) YIELD: data points Example: DTSQL Query (3) Capture periods where values are either above a hard-coded threshold or outside an (absolute) range of \u00b145 around a local average. Only consider periods that last less than 30 minutes. WITH SAMPLES: avg(\"\", \"2023-02-28T12:45:55-01:45\") AS localAvg USING EVENTS: OR(gt(300.25), NOT(around(abs, localAvg, 45))) FOR [0,30) minutes AS myEvent YIELD: all periods Example: DTSQL Query (4) Return the longest period where a monotonic decrease directly precedes a constant period. USING EVENTS: AND(const(10.0, 5.0)) AS constantEvent, AND(increase(5.25, 25.5, 600)) FOR (2,5] weeks AS increaseEvent SELECT PERIODS: (constantEvent precedes increaseEvent) YIELD: longest period Example: DTSQL Query (5) First apply a temporal filter, then detect periods where a constant period appears before a switch from values above to below the global average, with not more than two days in between. WITH SAMPLES: avg() AS globalAvg APPLY FILTER: AND(NOT(after(\"2019-08-29T10:00:00Z\"))) USING EVENTS: AND(const(10, 5)) AS constantEvent AND(lt(globalAvg)) FOR [20,] days AS low, AND(gt(globalAvg)) FOR [20,] days AS high SELECT PERIODS: (constantEvent precedes (low follows high) WITHIN [0,2] days) YIELD: all periods","title":"DTSQL Query Overview"},{"location":"query-language/overview/#dtsql-query-overview","text":"","title":"DTSQL Query Overview"},{"location":"query-language/overview/#notational-conventions","text":"This documentation follows (and goes beyond) common conventions known from usage messages (program synopsis), e.g., in UNIX man pages. They are summarized below. Abstract: Notational Conventions for Syntax Definitions Optional arguments [a] : a is optional. Alternative options [a|b] : May be left out, but if present, choose either a or b . Alternative required options {a|b} : Must not be left out; choose either a or b . Conditionally valid options [-a arg [-b]] : If -a arg is present, then optionally adding -b is also valid. Multiple occurrence a... : A range of a occurrences is allowed. The context must specify both whether cardinalities 0..n or 1..n are valid as well as the separator to use (e.g., whitespace or comma). Placeholders <a> : The placeholder <a> represents a concept that is explained later on or below. Lexicographic elements 'a' : Single-quoted text like 'a' stands for lexicographic elements to be used verbatim in a query (to distinguish them from syntax definition symbols). Whitespaces are implicitly understood as such.","title":"Notational Conventions"},{"location":"query-language/overview/#query-syntax","text":"The Getting Started page has already provided an exemplary query. The general structure of DTSQL queries is as follows: ['WITH SAMPLES:' {<GlobalValueAggregate>|<LocalValueAggregate>|<TemporalAggregate>} 'AS' <sampleIdentifier>...] ['APPLY FILTER:' {'AND('|'OR('}{'NOT('<filterFunction>')'|<filterFunction>}...')'] ['USING EVENTS:' {'AND('|'OR('}{'NOT('<eventFunction>')'|<eventFunction>}...')' ['FOR' {'('|'['} [<minLength>]',' [<maxLength>] {')'|']'} <unit>] 'AS' <eventIdentifier>...] ['SELECT PERIODS:' <selection>] 'YIELD:' {'all periods'|'longest period'|'shortest period'|'data points'|'sample' <sampleIdentifier>|'samples' <sampleIdentifier>...} As already mentioned before, all components (apart from YIELD ) are optional. Furthermore, whitespaces do not carry semantics. In other words, a DTSQL query may be formatted arbitrarily. There are instances, however, that require a whitespace (space, new line, tab), e.g., after the colon ( : ) at the beginning of each component.","title":"Query Syntax"},{"location":"query-language/overview/#examples","text":"In order to illustrate the syntax depicted above, the following paragraphs feature some (simple and not practice-oriented) exemplary queries along with their purpose. The next pages will explain how the elements of the components mentioned above, and contained in the sample queries, are defined exactly. Example: DTSQL Query (1) Capture and return global, local and temporal aggregates: WITH SAMPLES: avg() AS globalAverage, integral(\"2017-09-09T10:00:00Z\", \"\") AS integralTillTheEnd, sum_t(days, \"2017-09-09T10:00:00Z/2019-09-04T10:00:00Z\", \"2018-06-11T10:00:00Z/2019-01-08T10:00:00Z\", \"2019-06-27T10:00:00Z/2019-09-12T10:00:00Z\") AS totalDuration YIELD: samples globalAverage, integralTillTheEnd, totalDuration Example: DTSQL Query (2) Return data points after a temporal threshold which are greater than the global average and within a \u00b110 % range of the global maximum. WITH SAMPLES: avg() AS globalAvg, max() AS globalMax APPLY FILTER: AND(after(\"2018-11-12T07:12:59+03:30\"), gt(globalAvg), around(rel, globalMax, 10)) YIELD: data points Example: DTSQL Query (3) Capture periods where values are either above a hard-coded threshold or outside an (absolute) range of \u00b145 around a local average. Only consider periods that last less than 30 minutes. WITH SAMPLES: avg(\"\", \"2023-02-28T12:45:55-01:45\") AS localAvg USING EVENTS: OR(gt(300.25), NOT(around(abs, localAvg, 45))) FOR [0,30) minutes AS myEvent YIELD: all periods Example: DTSQL Query (4) Return the longest period where a monotonic decrease directly precedes a constant period. USING EVENTS: AND(const(10.0, 5.0)) AS constantEvent, AND(increase(5.25, 25.5, 600)) FOR (2,5] weeks AS increaseEvent SELECT PERIODS: (constantEvent precedes increaseEvent) YIELD: longest period Example: DTSQL Query (5) First apply a temporal filter, then detect periods where a constant period appears before a switch from values above to below the global average, with not more than two days in between. WITH SAMPLES: avg() AS globalAvg APPLY FILTER: AND(NOT(after(\"2019-08-29T10:00:00Z\"))) USING EVENTS: AND(const(10, 5)) AS constantEvent AND(lt(globalAvg)) FOR [20,] days AS low, AND(gt(globalAvg)) FOR [20,] days AS high SELECT PERIODS: (constantEvent precedes (low follows high) WITHIN [0,2] days) YIELD: all periods","title":"Examples"},{"location":"query-language/samples/","text":"Samples # A sample is a scalar, real value computed based on an input time series. The computation of a sample is never reliant upon other, previously computed samples (they are independent of each other). Essentially, samples are aggregate values that can be (re-)used to make other components of the query depend on them (e.g., to compose filters or define events ). They may also serve as overall return value of a query. The syntax of the samples section is as follows: ['WITH SAMPLES:' {<GlobalValueAggregate>|<LocalValueAggregate>|<TemporalAggregate>} 'AS' <sampleIdentifier>...] In other words, the section is optional, but if it is present, then one or more samples must be declared (with the comma , as separator). Each sample consists of an aggregate function and an identifier . Furthermore, there are three types of samples. Global value aggregates operate on the value component of all data points, local value aggregates only on those in a specifiable period, and temporal aggregates operate on the durations of several periods. The aggregate functions supported by DTSQL are introduced below. Abstract: DTSQL Aggregate Functions value temporal semantics max max_t maximum value/interval length min min_t minimum value/interval length avg avg_t arithmetic mean of values/interval lengths count count_t number of values/intervals sum sum_t sum of values/interval lengths integral n/a definite integral (\u201carea\u201d) of function modelled by input time series (with linear interpolation between data points) with time axis/time differences scaled to seconds stddev stddev_t population standard deviation of values/interval lengths The identifier must start with a letter, followed by an arbitrary number of digits and/or letters. Global Value Samples # Global value samples have the following syntax: <valueFunction>() AS <sampleIdentifier> The <valueFunction> placeholder ought to be substituted by a value aggregate function from the first column in the table above. The <sampleIdentifier> placeholder represents an identifier as explained above. The example below defines two global value samples, an arithmetic mean and the integral. Example: Global Value Samples WITH SAMPLES: avg() AS globalAvg, integral() AS globalIntegral Local Value Samples # Local value samples have the following syntax: <valueFunction>('\"'[<lowerBound>]'\"', '\"'[<upperBound]'\"'}) AS <sampleIdentifier> The <valueFunction> placeholder ought to be substituted by a value aggregate function from the first column in the table above. The placeholders <lowerBound> and <upperBound> are valid timestamps which denote the boundaries that should be considered during sample calculation. More precisely, only data points of the input time series are considered whose time component is not earlier than <lowerBound> , and also not later than <upperBound> . If <lowerBound> / <upperBound> is not present, then the used bound is the start/end timestamp of the input time series. The <sampleIdentifier> placeholder represents an identifier as explained above. The example below defines two local value samples, the standard deviation of values between March 4th 2021 and April 28th 2021 (both 12 o\u2019clock UTC), as well as the overall number of data points recorded starting with April 1st 2021. Example: Local Value Samples WITH SAMPLES: stddev(\"2021-03-04T12:00:00Z\", \"2021-04-28T12:00:00Z\") AS localSample1, count(\"2021-04-01T00:00:00Z\", \"\") AS localSample2 Temporal Samples # Local value samples have the following syntax: <temporalFunction>(<unit>, <interval>...) AS <sampleIdentifier> The <temporalFunction> placeholder ought to be substituted by a temporal aggregate function from the second column in the table above. The <unit> placeholder represents a supported time unit . The <interval> placeholder is a collection of intervals over whose durations the sample should be calculated. They are separated by commas ( , ) and expressed in an appropriate format . The <sampleIdentifier> placeholder represents an identifier as explained above. Warning: Irregularities With Temporal Samples There is no temporal sample corresponding to the integral function because the notion of integrals over interval duration is not reasonable. The count_t function does not take a unit argument because that would not make sense. The example below illustrates temporal samples by means of calculating the total duration of some periods as well as their number. Example: Temporal Samples WITH SAMPLES: sum_t(minutes, \"2022-08-28T17:00:00Z/2022-08-28T22:00:00Z\", \"2022-08-29T00:00:00Z/2022-08-29T02:00:00Z\", \"2022-08-29T05:00:00Z/2022-08-29T13:00:00Z\") AS totalDuration, count_t(\"2022-08-28T17:00:00Z/2022-08-28T22:00:00Z\", \"2022-08-29T00:00:00Z/2022-08-29T02:00:00Z\", \"2022-08-29T05:00:00Z/2022-08-29T13:00:00Z\") AS totalPeriods","title":"Samples"},{"location":"query-language/samples/#samples","text":"A sample is a scalar, real value computed based on an input time series. The computation of a sample is never reliant upon other, previously computed samples (they are independent of each other). Essentially, samples are aggregate values that can be (re-)used to make other components of the query depend on them (e.g., to compose filters or define events ). They may also serve as overall return value of a query. The syntax of the samples section is as follows: ['WITH SAMPLES:' {<GlobalValueAggregate>|<LocalValueAggregate>|<TemporalAggregate>} 'AS' <sampleIdentifier>...] In other words, the section is optional, but if it is present, then one or more samples must be declared (with the comma , as separator). Each sample consists of an aggregate function and an identifier . Furthermore, there are three types of samples. Global value aggregates operate on the value component of all data points, local value aggregates only on those in a specifiable period, and temporal aggregates operate on the durations of several periods. The aggregate functions supported by DTSQL are introduced below. Abstract: DTSQL Aggregate Functions value temporal semantics max max_t maximum value/interval length min min_t minimum value/interval length avg avg_t arithmetic mean of values/interval lengths count count_t number of values/intervals sum sum_t sum of values/interval lengths integral n/a definite integral (\u201carea\u201d) of function modelled by input time series (with linear interpolation between data points) with time axis/time differences scaled to seconds stddev stddev_t population standard deviation of values/interval lengths The identifier must start with a letter, followed by an arbitrary number of digits and/or letters.","title":"Samples"},{"location":"query-language/samples/#global-value-samples","text":"Global value samples have the following syntax: <valueFunction>() AS <sampleIdentifier> The <valueFunction> placeholder ought to be substituted by a value aggregate function from the first column in the table above. The <sampleIdentifier> placeholder represents an identifier as explained above. The example below defines two global value samples, an arithmetic mean and the integral. Example: Global Value Samples WITH SAMPLES: avg() AS globalAvg, integral() AS globalIntegral","title":"Global Value Samples"},{"location":"query-language/samples/#local-value-samples","text":"Local value samples have the following syntax: <valueFunction>('\"'[<lowerBound>]'\"', '\"'[<upperBound]'\"'}) AS <sampleIdentifier> The <valueFunction> placeholder ought to be substituted by a value aggregate function from the first column in the table above. The placeholders <lowerBound> and <upperBound> are valid timestamps which denote the boundaries that should be considered during sample calculation. More precisely, only data points of the input time series are considered whose time component is not earlier than <lowerBound> , and also not later than <upperBound> . If <lowerBound> / <upperBound> is not present, then the used bound is the start/end timestamp of the input time series. The <sampleIdentifier> placeholder represents an identifier as explained above. The example below defines two local value samples, the standard deviation of values between March 4th 2021 and April 28th 2021 (both 12 o\u2019clock UTC), as well as the overall number of data points recorded starting with April 1st 2021. Example: Local Value Samples WITH SAMPLES: stddev(\"2021-03-04T12:00:00Z\", \"2021-04-28T12:00:00Z\") AS localSample1, count(\"2021-04-01T00:00:00Z\", \"\") AS localSample2","title":"Local Value Samples"},{"location":"query-language/samples/#temporal-samples","text":"Local value samples have the following syntax: <temporalFunction>(<unit>, <interval>...) AS <sampleIdentifier> The <temporalFunction> placeholder ought to be substituted by a temporal aggregate function from the second column in the table above. The <unit> placeholder represents a supported time unit . The <interval> placeholder is a collection of intervals over whose durations the sample should be calculated. They are separated by commas ( , ) and expressed in an appropriate format . The <sampleIdentifier> placeholder represents an identifier as explained above. Warning: Irregularities With Temporal Samples There is no temporal sample corresponding to the integral function because the notion of integrals over interval duration is not reasonable. The count_t function does not take a unit argument because that would not make sense. The example below illustrates temporal samples by means of calculating the total duration of some periods as well as their number. Example: Temporal Samples WITH SAMPLES: sum_t(minutes, \"2022-08-28T17:00:00Z/2022-08-28T22:00:00Z\", \"2022-08-29T00:00:00Z/2022-08-29T02:00:00Z\", \"2022-08-29T05:00:00Z/2022-08-29T13:00:00Z\") AS totalDuration, count_t(\"2022-08-28T17:00:00Z/2022-08-28T22:00:00Z\", \"2022-08-29T00:00:00Z/2022-08-29T02:00:00Z\", \"2022-08-29T05:00:00Z/2022-08-29T13:00:00Z\") AS totalPeriods","title":"Temporal Samples"},{"location":"query-language/selection/","text":"Selection # The optional selection component utilizes selection operators to define temporal relations between the intervals represented by the events component . This makes it possible to create more complex, composite events by connecting the previously defined ( atomic ) events temporally using their identifiers. Temporal Relations # The figure below illustrates possible temporal relations between intervals: Allen\u2019s Interval Relations. Adapted From (Allen, 1983) 1 Selection Operators # In fact, DTSQL supports four of the thirteen relations introduced by Allen 1 : meets , met-by , before and after . This is realized by means of two selection operators precedes and follows which have an optional time-gap constraint. If this constraint is present, before / after semantics are used, otherwise meets / met-by . An instance of such an operator forms a binary event sequence. DTSQL supports n-ary sequences as well by allowing selection operators to be declared recursively. This is shown in the syntax of the selection component and its subcomponents below: ['SELECT PERIODS:' <operator> ['FOR' {'('|'['} [<minLength>]',' [<maxLength>] {')'|']'} <unit>]] Again, the selection component is optional, but if it is present, then a selection operator is compulsory. In this definition, in order to allow for proper recursion, the placeholder <operator> is defined as follows: '('{<eventIdentifier>|<operator>} {'follows'|'precedes'} {<eventIdentifier>|<operator>}')' The recursive base case is when both the left and right operator is an event identifier. Warning: Event Identifier vs. Sample Identifier It is prohibited to reference a sample within the selection component, only event identifiers are valid in this context. A DTSQL parser must throw an error if a sample is referenced by a selection operator. The previously mentioned optional time-gap constraint (starting with WITHIN ) changes the semantics employed during (composite) period detection. If it is present, an event sequence defined by a temporal relation is detected based on the length of the time-gap between the bounds of the participating periods ( before / after ). If it is not present, the temporal relation is deemed satisfied only if there are no data points in between the ( meets / met-by ). Warning: Absent Time-Gap Constraint vs. WITHIN [,] A consequence of the paragraph above is that, unlike for duration constraints with filters , a time-gap of [,] (arbitrary unit) is not equivalent to no such constraint being presen at all. If no time-gap constraint is present, it means that there must not be a single data point in between the end and start of periods forming a (binary) event sequence ( meets / met-by semantics). On the other hand, a time-gap constraint of WITHIN [,] <unit> is equivalent to WITHIN [0,] <unit> which defines an acceptable range of 0 to infinite <unit> between periods composing a (binary) event sequence. In other words, the second variant denotes that the time-gap may be arbitrarily large, which, in fact, represents the conceptual opposite of leaving out the constraint. Examples # Example: Selection Component (1) Assume a query with events e1 , e2 , e3 with correspondingly detected periods as depicted in the figure below. The shading of the sections between intervals indicates whether there are data points recorded in those gaps or not (\u201cimmediate\u201d). Now assume the following selection component: SELECT PERIODS: (e1 follows (e2 precedes e3 WITHIN [0,15] minutes)) The (recursive) evaluation of the selection operators yields that the nested (e2 precedes e3 WITHIN [0,15] minutes) represents the composite intervals t3-t6 as well as t9-t12 . Based on that, the overall selection evaluation yields the composite interval t9-t14 because this is the only instance where event e1 immediately follows a period from the nested operator. Example: Selection Component (2) Assume events e1 , e2 and e4 . Furthermore, assume that you want to capture ternary event sequences where e2 appears before a e1 -> e4 pair with not more than 30 minutes apart. The corresponding selection component is depicted below. USING EVENTS: (...) AS e1, (...) AS e2, (...) AS e4 SELECT PERIODS: (e2 precedes (e4 follows e1) WITHIN [0,30] minutes) Example: Selection Component (3) Assume a query with events e1 , e2 , e3 , e4 . Furthermore, suppose three scenarios (a) , (b) , (c) where the evaluation of the events component yields event sequences as depicted in the diagram below. An arrow without text depicts an interval transition without data points in between, an arrow with time information merely states the time difference between the bounds of the intervals involved (there may be data points in between or not). stateDiagram-v2 state (a) { direction LR e2_a: e2 e1_a: e1 e4_a: e4 e2_a --> e1_a e1_a --> e4_a } state (b) { direction LR e4_b: e4 e2_b: e2 e1_b: e1 e2_b --> e1_b: 40 min e2_b --> ... ... --> e1_b e1_b --> e4_b } state (c) { direction LR e1_c: e1 e3_c: e3 e4_c: e4 e3_c --> e1_c e1_c --> e4_c } Now, consider four different selection components: S1: SELECT PERIODS: (e4 follows (e1 follows e2)) S2: SELECT PERIODS: (e4 follows (e1 follows e2 WITHIN [0,45] minutes)) S3: SELECT PERIODS: (e4 follows e1) S4: SELECT PERIODS: (e2 precedes (e4 follows e1) WITHIN [0,30] minutes) S1 represents scenario (a) , but neither (b) nor (c) . S2 describes scenario (b) , and maybe (a) , depending on the time-gap between e2 and e1 . S3 characterizes all scenarios (a) , (b) , (c) . Finally, S4 is not guaranteed to match any scenario: Only (a) could potentially fit, if the end of e2 and the start of e1 are not more than 30 minutes apart. James F. Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM 26, 11 (Nov. 1983), 832\u2013843. https://doi.org/10.1145/182.358434 \u21a9 \u21a9","title":"Selection"},{"location":"query-language/selection/#selection","text":"The optional selection component utilizes selection operators to define temporal relations between the intervals represented by the events component . This makes it possible to create more complex, composite events by connecting the previously defined ( atomic ) events temporally using their identifiers.","title":"Selection"},{"location":"query-language/selection/#temporal-relations","text":"The figure below illustrates possible temporal relations between intervals: Allen\u2019s Interval Relations. Adapted From (Allen, 1983) 1","title":"Temporal Relations"},{"location":"query-language/selection/#selection-operators","text":"In fact, DTSQL supports four of the thirteen relations introduced by Allen 1 : meets , met-by , before and after . This is realized by means of two selection operators precedes and follows which have an optional time-gap constraint. If this constraint is present, before / after semantics are used, otherwise meets / met-by . An instance of such an operator forms a binary event sequence. DTSQL supports n-ary sequences as well by allowing selection operators to be declared recursively. This is shown in the syntax of the selection component and its subcomponents below: ['SELECT PERIODS:' <operator> ['FOR' {'('|'['} [<minLength>]',' [<maxLength>] {')'|']'} <unit>]] Again, the selection component is optional, but if it is present, then a selection operator is compulsory. In this definition, in order to allow for proper recursion, the placeholder <operator> is defined as follows: '('{<eventIdentifier>|<operator>} {'follows'|'precedes'} {<eventIdentifier>|<operator>}')' The recursive base case is when both the left and right operator is an event identifier. Warning: Event Identifier vs. Sample Identifier It is prohibited to reference a sample within the selection component, only event identifiers are valid in this context. A DTSQL parser must throw an error if a sample is referenced by a selection operator. The previously mentioned optional time-gap constraint (starting with WITHIN ) changes the semantics employed during (composite) period detection. If it is present, an event sequence defined by a temporal relation is detected based on the length of the time-gap between the bounds of the participating periods ( before / after ). If it is not present, the temporal relation is deemed satisfied only if there are no data points in between the ( meets / met-by ). Warning: Absent Time-Gap Constraint vs. WITHIN [,] A consequence of the paragraph above is that, unlike for duration constraints with filters , a time-gap of [,] (arbitrary unit) is not equivalent to no such constraint being presen at all. If no time-gap constraint is present, it means that there must not be a single data point in between the end and start of periods forming a (binary) event sequence ( meets / met-by semantics). On the other hand, a time-gap constraint of WITHIN [,] <unit> is equivalent to WITHIN [0,] <unit> which defines an acceptable range of 0 to infinite <unit> between periods composing a (binary) event sequence. In other words, the second variant denotes that the time-gap may be arbitrarily large, which, in fact, represents the conceptual opposite of leaving out the constraint.","title":"Selection Operators"},{"location":"query-language/selection/#examples","text":"Example: Selection Component (1) Assume a query with events e1 , e2 , e3 with correspondingly detected periods as depicted in the figure below. The shading of the sections between intervals indicates whether there are data points recorded in those gaps or not (\u201cimmediate\u201d). Now assume the following selection component: SELECT PERIODS: (e1 follows (e2 precedes e3 WITHIN [0,15] minutes)) The (recursive) evaluation of the selection operators yields that the nested (e2 precedes e3 WITHIN [0,15] minutes) represents the composite intervals t3-t6 as well as t9-t12 . Based on that, the overall selection evaluation yields the composite interval t9-t14 because this is the only instance where event e1 immediately follows a period from the nested operator. Example: Selection Component (2) Assume events e1 , e2 and e4 . Furthermore, assume that you want to capture ternary event sequences where e2 appears before a e1 -> e4 pair with not more than 30 minutes apart. The corresponding selection component is depicted below. USING EVENTS: (...) AS e1, (...) AS e2, (...) AS e4 SELECT PERIODS: (e2 precedes (e4 follows e1) WITHIN [0,30] minutes) Example: Selection Component (3) Assume a query with events e1 , e2 , e3 , e4 . Furthermore, suppose three scenarios (a) , (b) , (c) where the evaluation of the events component yields event sequences as depicted in the diagram below. An arrow without text depicts an interval transition without data points in between, an arrow with time information merely states the time difference between the bounds of the intervals involved (there may be data points in between or not). stateDiagram-v2 state (a) { direction LR e2_a: e2 e1_a: e1 e4_a: e4 e2_a --> e1_a e1_a --> e4_a } state (b) { direction LR e4_b: e4 e2_b: e2 e1_b: e1 e2_b --> e1_b: 40 min e2_b --> ... ... --> e1_b e1_b --> e4_b } state (c) { direction LR e1_c: e1 e3_c: e3 e4_c: e4 e3_c --> e1_c e1_c --> e4_c } Now, consider four different selection components: S1: SELECT PERIODS: (e4 follows (e1 follows e2)) S2: SELECT PERIODS: (e4 follows (e1 follows e2 WITHIN [0,45] minutes)) S3: SELECT PERIODS: (e4 follows e1) S4: SELECT PERIODS: (e2 precedes (e4 follows e1) WITHIN [0,30] minutes) S1 represents scenario (a) , but neither (b) nor (c) . S2 describes scenario (b) , and maybe (a) , depending on the time-gap between e2 and e1 . S3 characterizes all scenarios (a) , (b) , (c) . Finally, S4 is not guaranteed to match any scenario: Only (a) could potentially fit, if the end of e2 and the start of e1 are not more than 30 minutes apart. James F. Allen. 1983. Maintaining knowledge about temporal intervals. Communications of the ACM 26, 11 (Nov. 1983), 832\u2013843. https://doi.org/10.1145/182.358434 \u21a9 \u21a9","title":"Examples"},{"location":"query-language/yield/","text":"Yield # The yield statement ultimately determines the result of a DTSQL query. Its syntax along with the supported yield formats are defined as follows: 'YIELD:' {'all periods'|'longest period'|'shortest period'|'data points'|'sample' <sampleIdentifier>|'samples' <sampleIdentifier>...} Unlike the other components, the yield section is mandatory. It determines one of six different result collection semantics to be used: all periods : If neither events nor selection component is present, an empty range of intervals is returned. If only the events component is present, then the set of captured event intervals is returned. If both events and selection component are present, all composite intervals are returned. longest period : Returns the first period (i.e., the one with the earliest start date) whose length is equal to the maximum duration of captured periods. shortest period : Returns the first period (i.e., the one with the earliest start date) whose length is equal to the minimum duration of captured periods. data points : All data points that have not been filtered out and/or are contained in a t least one detected interval are returned. sample : The computed value of the sample referenced by the placeholder <sampleIdentifier> is returned. samples : The computed values of the samples referenced by the placeholders <sampleIdentifier> (separated by comma , ) are returned. The first example below returns the number of data points as well as the definite integral in the month of November in 2022. Example: Yield Samples WITH SAMPLES: integral(\"2022-11-01T00:00:00Z\", \"2022-11-30T23:59:59.999Z\") AS novemberIntegral, count(\"2022-11-01T00:00:00Z\", \"2022-11-30T23:59:59.999Z\") AS novemberCount YIELD: samples novemberIntegral, novemberCount The second example below returns all periods that exhibit values that are continuously between 245.34 (inclusively) and the global average (exclusively). Example: Yield All Periods WITH SAMPLES: avg() AS myAvg USING EVENTS: AND(NOT(lt(245.34)), lt(myAvg)) AS inRange YIELD: all periods","title":"Yield"},{"location":"query-language/yield/#yield","text":"The yield statement ultimately determines the result of a DTSQL query. Its syntax along with the supported yield formats are defined as follows: 'YIELD:' {'all periods'|'longest period'|'shortest period'|'data points'|'sample' <sampleIdentifier>|'samples' <sampleIdentifier>...} Unlike the other components, the yield section is mandatory. It determines one of six different result collection semantics to be used: all periods : If neither events nor selection component is present, an empty range of intervals is returned. If only the events component is present, then the set of captured event intervals is returned. If both events and selection component are present, all composite intervals are returned. longest period : Returns the first period (i.e., the one with the earliest start date) whose length is equal to the maximum duration of captured periods. shortest period : Returns the first period (i.e., the one with the earliest start date) whose length is equal to the minimum duration of captured periods. data points : All data points that have not been filtered out and/or are contained in a t least one detected interval are returned. sample : The computed value of the sample referenced by the placeholder <sampleIdentifier> is returned. samples : The computed values of the samples referenced by the placeholders <sampleIdentifier> (separated by comma , ) are returned. The first example below returns the number of data points as well as the definite integral in the month of November in 2022. Example: Yield Samples WITH SAMPLES: integral(\"2022-11-01T00:00:00Z\", \"2022-11-30T23:59:59.999Z\") AS novemberIntegral, count(\"2022-11-01T00:00:00Z\", \"2022-11-30T23:59:59.999Z\") AS novemberCount YIELD: samples novemberIntegral, novemberCount The second example below returns all periods that exhibit values that are continuously between 245.34 (inclusively) and the global average (exclusively). Example: Yield All Periods WITH SAMPLES: avg() AS myAvg USING EVENTS: AND(NOT(lt(245.34)), lt(myAvg)) AS inRange YIELD: all periods","title":"Yield"}]}